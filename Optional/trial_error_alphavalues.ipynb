{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f6770e3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sympy as sp\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a23528ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 0.1, 0.01, 0.001, 0.0001, 1e-05, 1e-06, 1e-07, 1e-08, 1e-09]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Alpha values to try:\n",
    "# (we're assuming alpha=1 would cause divergence, not always the case though)\n",
    "\n",
    "a_highestorder = 0\n",
    "a_lowestorder = -10\n",
    "alphas = []\n",
    "\n",
    "for i in range(a_highestorder, a_lowestorder, -1):\n",
    "    alphas.append(10**i)\n",
    "\n",
    "alphas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2b699963",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The Data\n",
    "\n",
    "x0 = [2, 5, 9]\n",
    "x1 = [31, 56, 72]\n",
    "ya = [240, 430, 562]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0b6e2247",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guessing initial weights and bias\n",
    "\n",
    "w = [1, 2]\n",
    "b = 3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9b3e2fcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create symbols need\n",
    "\n",
    "w_s = sp.symbols('w0:2')\n",
    "b_s = sp.symbols('b')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d7359106",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create cost symbolically\n",
    "\n",
    "cost = 0\n",
    "for i in range(0, len(ya)):\n",
    "    diff = w_s[0]*x0[i] + w_s[1]*x1[i] + b_s - ya[i]\n",
    "    cost += diff**2\n",
    "cost = cost / (2*3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6eee9d5b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$\\displaystyle 48478$"
      ],
      "text/plain": [
       "48478"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Figure out initial cost\n",
    "w_b_vals = [ (w_s[0], w[0]), (w_s[1], w[1]), (b_s, b)]\n",
    "round(cost.subs(w_b_vals))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "318248a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Work out gradient of cost\n",
    "\n",
    "grad_cost = []\n",
    "for i in range(0, len(w)):\n",
    "    dC_dwi = sp.diff(cost, w_s[i])\n",
    "    grad_cost.append(dC_dwi)\n",
    "\n",
    "dC_db = sp.diff(cost, b_s)\n",
    "grad_cost.append(dC_db)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "12a863b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alpha 1 is causing divergence, moving onto next value\n",
      "alpha 0.1 is causing divergence, moving onto next value\n",
      "alpha 0.01 is causing divergence, moving onto next value\n",
      "alpha 0.001 is causing divergence, moving onto next value\n",
      "This alpha is the best: 0.0001\n"
     ]
    }
   ],
   "source": [
    "# The best alpha in this trial and error approach, will be the first one that DOES NOT diverge:\n",
    "\n",
    "\n",
    "# Try out each alpha:\n",
    "for j in range(0, len(alphas)):\n",
    "    \n",
    "    \n",
    "    # Reset weights/bias\n",
    "    w = [1, 2]\n",
    "    b = 3\n",
    "    costs_alpha = []\n",
    "    isdivergent = False\n",
    "    \n",
    "    \n",
    "    # Perform 1000 laps of training\n",
    "    for i in range(0, 1000):\n",
    "\n",
    "        # Pair the weights/bias, with their actual values\n",
    "        w_b_vals = [ (w_s[0], w[0]), (w_s[1], w[1]), (b_s, b)]\n",
    "        \n",
    "        # Calculate cost\n",
    "        cost_n = cost.subs(w_b_vals)\n",
    "        costs_alpha.append(cost_n)\n",
    "        \n",
    "        # Check if cost is diverging\n",
    "        if costs_alpha[i] > costs_alpha[i-1]:\n",
    "            isdivergent = True\n",
    "            print(\"alpha {} is causing divergence, moving onto next value\".format(alphas[j]))\n",
    "            break\n",
    "\n",
    "        # Substitute values into all partial derivatives\n",
    "        grad_cost_n = []\n",
    "        for i in range(0, len(grad_cost)):\n",
    "            deriv_n = grad_cost[i].subs(w_b_vals)\n",
    "            grad_cost_n.append(deriv_n)\n",
    "\n",
    "        # Update/change weights and bias\n",
    "        w[0] = w[0] + alphas[j]*-grad_cost_n[0]\n",
    "        w[1] = w[1] + alphas[j]*-grad_cost_n[1]\n",
    "        b = b + alphas[j] * -grad_cost_n[2]\n",
    "    \n",
    "    # If no break occured, then we've found the largest alpha that did not diverge, thus it is the best\n",
    "    if isdivergent == False:\n",
    "        print(\"This alpha is the best: {}\".format(alphas[j]))\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f6c8267",
   "metadata": {},
   "source": [
    "Note - Optimizing trial and error search:\n",
    "\n",
    "To help reduce how long it takes to find a decent value for alpha, we calculate the cost during each lap of training, for each alpha value, and if the cost from a particular lap, goes up compared to the previous lap, we immediately know that for all subsequent laps (jumps), cost will only continue to go up (divergence will continue), so we know that this alpha value will cause divergence, and thus we'll stop training with it, and instead move onto the next alpha value.\n",
    "\n",
    "This means we only need to do two laps of training for each diverging alpha value, then 1000 laps for the first non-diverging alpha value (compared to 1000 laps for EVERY alpha value, and then just compare cost for each, and see which was lowest).\n",
    "\n",
    "HOWEVER, this optimization to trial and error finding for a decent alpha value, DOES NOT WORK, for NON-LINEAR regression models (as the graphs of cost in linear regression models, are always either parabolic, or the parabolic equivalent for higher dimensions, which have simple geometry, so that one jump of divergence, means all future jumps will also be divergent. Non-linear regression models DO NOT necessarily follow this behaviour)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
