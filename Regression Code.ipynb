{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d085ce21",
   "metadata": {},
   "source": [
    "## Multivariable Linear Regression - From Scratch\n",
    "\n",
    "<br><br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11612337",
   "metadata": {},
   "source": [
    "This project is about **coding from scratch, a machine learning algorithm, that can perform multivariable linear regression** (MLR).\n",
    "\n",
    "There is a heavy focus on explaining what's going on, step by step, and the code/comments are written as simply as I could make them, to help demonstrate my understanding of machine learning, and MLR. \n",
    "\n",
    "There is also a PDF in the same folder as this file,  that explains the underlying mathematics (\"MLR Explanation - Includes Math.pdf\")\n",
    "\n",
    "Since the code/comments in this project are fairly basic, the overall project is more suited towards anyone wanting to gauge how well I understand the basics of machine learning, or for anyone who may not be too familiar with how machine learning handles multivariable linear regression, and what the underlying code/mathematics is like.\n",
    "\n",
    "Now, lets begin:\n",
    "\n",
    "<br>\n",
    "\n",
    "*Note: All raw data involved is randomly generated, and is quantitative in nature (so, really simple data). The reason for this, is to ensure that focus is kept on understanding how machine learning works here, as opposed to how to properly clean data, and perform feature engineering and scaling.*\n",
    "\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2a7c9bf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is a from scratch project, but we do briefly use the random and pandas modules, just to generate some raw data and \n",
    "# display it neatly. \n",
    "import random as rm\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# Additionally, SymPy IS used, but ONLY to help demonstrate that the from scratch code works correctly.\n",
    "import sympy as sp\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05baf282",
   "metadata": {},
   "source": [
    "<br><br><br>\n",
    "Now, first, we'll determine how much data we have, specifically the number of independent variables (also known as the \n",
    "features/characteristics of something) and the number of instances of data, (how many of that something we have).\n",
    "\n",
    "(For example, if our data is about houses, and we have 12 independent variables, and 100 instances of data,  then that means we have 100 houses in our data, and the same 12 things have been measured/found for each one of these houses, like: \"land size\", \"value\", front footage\", \"number of bedrooms\", etc)\n",
    "\n",
    "For this demonstration, I'll be going with 12 independent variables, and 100 instances of data, but this can be changed\n",
    "to your liking (more data means a longer runtime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "34e3caa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_vars = 12\n",
    "n_instncs = 100\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c05d5e8f",
   "metadata": {},
   "source": [
    "<br><br><br>\n",
    "Now, generating values for each of the independent variables. \n",
    "\n",
    "The values for all variables will be stored in a list of lists. \n",
    " \n",
    "Also creating \"scales\", which will have a bunch of different scales (ranging from millionths, to millions), to help ensure \n",
    "our variables are on different scales relative to each other, as commonly found in real life data (for example,\n",
    "the variable \"house_value\", is generally on the scale of 100,000 (1e5), to 1,000,000 (1e6), whereas\n",
    "\"house_age\" is generally on the scale of 1 (1e0), to 100 (1e2)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "faec1801",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data = []\n",
    "scales = [1e-6, 1e-5, 1e-4, 1e-3, 1e-2, 1e-1, 1e0, 1e1, 1e2, 1e3, 1e4, 1e5, 1e6]\n",
    "\n",
    "\n",
    "# Iterate through each independent variable:\n",
    "for i in range(0, n_vars):\n",
    "    \n",
    "    # Create a list within data, for that variable\n",
    "    data.append([])\n",
    "\n",
    "    # Randomly generate values for that variable, ensuring it has a random scale and range.\n",
    "    \n",
    "    # Pick the starting scale (can't be millions, otherwise that variable will be full of identical values)\n",
    "    n = rm.randint(0, len(scales)-2)\n",
    "    # Pick the final scale (must be larger than starting scale)\n",
    "    m = rm.randint(n+1, len(scales)-1)\n",
    "    \n",
    "    min = scales[n]\n",
    "    max = scales[m]\n",
    "    \n",
    "    for j in range(0, n_instncs):\n",
    "        rndm_num = rm.uniform(min, max)\n",
    "        data[i].append(rndm_num)\n",
    "\n",
    "        \n",
    "        \n",
    "# Note: \"scales\" could have been created using a list comprehension (geometric sequence), but as this project focuses on \n",
    "# simplicity and explanation, a simpler, less scalable approach was used.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78ac78ed",
   "metadata": {},
   "source": [
    "<br><br><br>\n",
    "Now, we'll take a look at our data, and see if the variables are actually on different scales relative to each other.\n",
    "\n",
    "*Note: We'll put our data in a pandas dataframe SOLELY for presentation/aesthetic purposes, then delete the dataframe, as this is a \"from scratch\" project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "66830105",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x0</th>\n",
       "      <th>x1</th>\n",
       "      <th>x2</th>\n",
       "      <th>x3</th>\n",
       "      <th>x4</th>\n",
       "      <th>x5</th>\n",
       "      <th>x6</th>\n",
       "      <th>x7</th>\n",
       "      <th>x8</th>\n",
       "      <th>x9</th>\n",
       "      <th>x10</th>\n",
       "      <th>x11</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4.06E+04</td>\n",
       "      <td>3.23E+01</td>\n",
       "      <td>7.31E+05</td>\n",
       "      <td>8.86E+03</td>\n",
       "      <td>5.84E+04</td>\n",
       "      <td>6.26E+02</td>\n",
       "      <td>6.97E-02</td>\n",
       "      <td>2.91E+04</td>\n",
       "      <td>2.12E+05</td>\n",
       "      <td>3.64E+00</td>\n",
       "      <td>4.68E-04</td>\n",
       "      <td>8.75E+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7.77E+04</td>\n",
       "      <td>5.77E+01</td>\n",
       "      <td>8.55E+05</td>\n",
       "      <td>7.41E+03</td>\n",
       "      <td>5.37E+04</td>\n",
       "      <td>6.52E+02</td>\n",
       "      <td>2.50E-02</td>\n",
       "      <td>2.15E+04</td>\n",
       "      <td>7.12E+05</td>\n",
       "      <td>5.43E+00</td>\n",
       "      <td>7.57E-04</td>\n",
       "      <td>4.94E+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.51E+04</td>\n",
       "      <td>4.12E+01</td>\n",
       "      <td>7.70E+05</td>\n",
       "      <td>3.57E+03</td>\n",
       "      <td>6.12E+04</td>\n",
       "      <td>6.80E+02</td>\n",
       "      <td>7.46E-02</td>\n",
       "      <td>2.73E+04</td>\n",
       "      <td>4.79E+05</td>\n",
       "      <td>3.00E+00</td>\n",
       "      <td>1.58E-04</td>\n",
       "      <td>5.39E+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7.55E+04</td>\n",
       "      <td>7.83E+01</td>\n",
       "      <td>5.01E+05</td>\n",
       "      <td>6.86E+03</td>\n",
       "      <td>6.72E+04</td>\n",
       "      <td>1.36E+02</td>\n",
       "      <td>8.47E-02</td>\n",
       "      <td>6.97E+04</td>\n",
       "      <td>1.96E+05</td>\n",
       "      <td>5.24E+00</td>\n",
       "      <td>1.32E-04</td>\n",
       "      <td>4.18E+04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6.18E+04</td>\n",
       "      <td>9.82E+01</td>\n",
       "      <td>3.24E+05</td>\n",
       "      <td>8.35E+03</td>\n",
       "      <td>6.58E+04</td>\n",
       "      <td>6.70E+02</td>\n",
       "      <td>5.67E-02</td>\n",
       "      <td>5.55E+04</td>\n",
       "      <td>5.03E+05</td>\n",
       "      <td>2.25E+00</td>\n",
       "      <td>4.38E-04</td>\n",
       "      <td>3.27E+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>6.30E+03</td>\n",
       "      <td>2.80E+01</td>\n",
       "      <td>6.85E+05</td>\n",
       "      <td>3.04E+03</td>\n",
       "      <td>7.28E+04</td>\n",
       "      <td>1.08E+02</td>\n",
       "      <td>8.71E-02</td>\n",
       "      <td>1.74E+04</td>\n",
       "      <td>1.49E+05</td>\n",
       "      <td>3.43E+00</td>\n",
       "      <td>9.37E-04</td>\n",
       "      <td>8.81E+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>2.06E+04</td>\n",
       "      <td>8.41E+01</td>\n",
       "      <td>2.39E+05</td>\n",
       "      <td>4.81E+03</td>\n",
       "      <td>3.85E+04</td>\n",
       "      <td>3.21E+02</td>\n",
       "      <td>4.58E-02</td>\n",
       "      <td>5.94E+04</td>\n",
       "      <td>8.50E+05</td>\n",
       "      <td>1.93E+00</td>\n",
       "      <td>4.15E-04</td>\n",
       "      <td>9.25E+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>5.02E+03</td>\n",
       "      <td>9.13E+01</td>\n",
       "      <td>9.73E+05</td>\n",
       "      <td>2.02E+03</td>\n",
       "      <td>1.03E+04</td>\n",
       "      <td>3.95E+02</td>\n",
       "      <td>5.98E-02</td>\n",
       "      <td>4.76E+04</td>\n",
       "      <td>3.17E+05</td>\n",
       "      <td>1.43E+00</td>\n",
       "      <td>6.92E-04</td>\n",
       "      <td>3.51E+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>8.31E+04</td>\n",
       "      <td>8.44E+01</td>\n",
       "      <td>1.08E+05</td>\n",
       "      <td>4.60E+03</td>\n",
       "      <td>6.48E+04</td>\n",
       "      <td>9.40E+02</td>\n",
       "      <td>8.58E-02</td>\n",
       "      <td>9.24E+04</td>\n",
       "      <td>4.92E+05</td>\n",
       "      <td>8.15E+00</td>\n",
       "      <td>3.00E-04</td>\n",
       "      <td>3.66E+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>7.37E+04</td>\n",
       "      <td>4.86E+01</td>\n",
       "      <td>5.04E+05</td>\n",
       "      <td>3.45E+03</td>\n",
       "      <td>6.44E+04</td>\n",
       "      <td>6.08E+02</td>\n",
       "      <td>9.16E-02</td>\n",
       "      <td>3.40E+04</td>\n",
       "      <td>8.92E+05</td>\n",
       "      <td>8.51E+00</td>\n",
       "      <td>5.22E-04</td>\n",
       "      <td>5.02E+05</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows Ã— 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         x0       x1       x2       x3       x4       x5       x6       x7  \\\n",
       "0  4.06E+04 3.23E+01 7.31E+05 8.86E+03 5.84E+04 6.26E+02 6.97E-02 2.91E+04   \n",
       "1  7.77E+04 5.77E+01 8.55E+05 7.41E+03 5.37E+04 6.52E+02 2.50E-02 2.15E+04   \n",
       "2  1.51E+04 4.12E+01 7.70E+05 3.57E+03 6.12E+04 6.80E+02 7.46E-02 2.73E+04   \n",
       "3  7.55E+04 7.83E+01 5.01E+05 6.86E+03 6.72E+04 1.36E+02 8.47E-02 6.97E+04   \n",
       "4  6.18E+04 9.82E+01 3.24E+05 8.35E+03 6.58E+04 6.70E+02 5.67E-02 5.55E+04   \n",
       "..      ...      ...      ...      ...      ...      ...      ...      ...   \n",
       "95 6.30E+03 2.80E+01 6.85E+05 3.04E+03 7.28E+04 1.08E+02 8.71E-02 1.74E+04   \n",
       "96 2.06E+04 8.41E+01 2.39E+05 4.81E+03 3.85E+04 3.21E+02 4.58E-02 5.94E+04   \n",
       "97 5.02E+03 9.13E+01 9.73E+05 2.02E+03 1.03E+04 3.95E+02 5.98E-02 4.76E+04   \n",
       "98 8.31E+04 8.44E+01 1.08E+05 4.60E+03 6.48E+04 9.40E+02 8.58E-02 9.24E+04   \n",
       "99 7.37E+04 4.86E+01 5.04E+05 3.45E+03 6.44E+04 6.08E+02 9.16E-02 3.40E+04   \n",
       "\n",
       "         x8       x9      x10      x11  \n",
       "0  2.12E+05 3.64E+00 4.68E-04 8.75E+05  \n",
       "1  7.12E+05 5.43E+00 7.57E-04 4.94E+05  \n",
       "2  4.79E+05 3.00E+00 1.58E-04 5.39E+05  \n",
       "3  1.96E+05 5.24E+00 1.32E-04 4.18E+04  \n",
       "4  5.03E+05 2.25E+00 4.38E-04 3.27E+05  \n",
       "..      ...      ...      ...      ...  \n",
       "95 1.49E+05 3.43E+00 9.37E-04 8.81E+05  \n",
       "96 8.50E+05 1.93E+00 4.15E-04 9.25E+05  \n",
       "97 3.17E+05 1.43E+00 6.92E-04 3.51E+05  \n",
       "98 4.92E+05 8.15E+00 3.00E-04 3.66E+05  \n",
       "99 8.92E+05 8.51E+00 5.22E-04 5.02E+05  \n",
       "\n",
       "[100 rows x 12 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creating a dataframe from our data. We created variable by variable, NOT instance by instance, so the pandas dataframe\n",
    "# will have each variable be a row, not a column, which is against convention, so we'll have to swap rows and columns \n",
    "# (transpose).\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "df = df.transpose()\n",
    "\n",
    "# Giving the columns (variables), the correct names, for aesthetic/clarity:\n",
    "df.columns=[\"x{}\".format(i) for i in range(0, 12)]\n",
    "\n",
    "# Displaying values in scientific notation, to 2 decimal places\n",
    "pd.set_option('display.float_format', '{:.2E}'.format)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11c045f6",
   "metadata": {},
   "source": [
    "<br><br><br>\n",
    "As we can see from our data above, the variables are on different scales relative to each other.\n",
    "(Some are on the higher scales, some are on the lower scales, which ones specifically is determined randomly)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "800ae94e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We no longer need the dataframe (we only used it for aesthetic presentation), so we'll now delete it.\n",
    "del df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7d617ec",
   "metadata": {},
   "source": [
    "<br><br><br>\n",
    "Now that we have our randomly generated raw data, we'll create a completely MADE UP mathematical relationship, between our \n",
    "independent variables (x0, x1, x2, etc), and our dependent variable, which we'll call y. \n",
    "\n",
    "This relationship will be LINEAR, so it'll look like this: y = a*x0 + c*x1 + d*x2 + e*x3 + ... + b, \n",
    "\n",
    "All the coefficients (a, c, d, e, etc) will be randomly generated, from a range of 1 to 100, as well as the constant \"k\" at the end. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "09f0e1d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "coeffs = []\n",
    "\n",
    "# Randomly generate coefficients\n",
    "for i in range(0, n_vars):\n",
    "    coeffs.append(rm.randint(1, 100))\n",
    "\n",
    "# Randomly generate constant\n",
    "k = rm.randint(1, 100)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78e6fde5",
   "metadata": {},
   "source": [
    "<br><br><br>\n",
    "So, our randomly generated, MADE UP mathematical relationship, is shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b00f25cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The made up mathematical relationship is:\n",
      "\n",
      "y = 93*x0 + 9*x1 + 7*x2 + 82*x3 + 3*x4 + 25*x5 + 3*x6 + 93*x7 + 1*x8 + 85*x9 + 41*x10 + 27*x11 + 25\n"
     ]
    }
   ],
   "source": [
    "rel_str = \"y =\"\n",
    "for i in range(0, len(coeffs)):\n",
    "    rel_str += \" {}*x{} +\".format(coeffs[i], i)\n",
    "\n",
    "rel_str += \" {}\".format(k)\n",
    "print(\"The made up mathematical relationship is:\\n\")\n",
    "print(rel_str)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e596cf78",
   "metadata": {},
   "source": [
    "<br><br><br>\n",
    "Now that we've generated all the coefficients and the constant, we can calculate the values for the made up dependent variable, y:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8a560131",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The first 5 values of y are: \n",
      "\n",
      "['3.64E+07', '3.01E+07', '2.48E+07', '1.91E+07', '2.34E+07']\n"
     ]
    }
   ],
   "source": [
    "dep_data = []\n",
    "\n",
    "# Each instance is a row in our dataframe:\n",
    "for i in range(0, n_instncs):\n",
    "    \n",
    "    dep_value = 0\n",
    "    \n",
    "    # Iterate through a specific column/instance\n",
    "    for j in range(0, n_vars):\n",
    "        \n",
    "        term = coeffs[j]*data[j][i]\n",
    "        dep_value += term\n",
    "    \n",
    "    dep_value += k\n",
    "    dep_data.append(dep_value)\n",
    "    \n",
    "    \n",
    "# Displaying the first 5 values of y:\n",
    "print(\"The first 5 values of y are: \\n\")\n",
    "print([\"{:.2E}\".format(n) for n in dep_data[:5]])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc2b0c1c",
   "metadata": {},
   "source": [
    "<br><br><br>\n",
    "Now, we bring our machine learning algorithm into play. It's supposed to create a model, that can predict what the dependent variable y is. First, we'll have it randomly guess what the coefficients are (conventionally referred to as weights now, so w0, w1, w2, w3, etc), as well as the constant \"k\" (conventionally referred to as bias now)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "39911014",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The prediction model:\n",
      "y = 23*x0 + 40*x1 + 72*x2 + 43*x3 + 54*x4 + 56*x5 + 10*x6 + 2*x7 + 1*x8 + 1*x9 + 95*x10 + 95*x11 + 35\n",
      "\n",
      "The actual mathematical relationship is: \n",
      "y = 93*x0 + 9*x1 + 7*x2 + 82*x3 + 3*x4 + 25*x5 + 3*x6 + 93*x7 + 1*x8 + 85*x9 + 41*x10 + 27*x11 + 25\n"
     ]
    }
   ],
   "source": [
    "guess_weights = []\n",
    "\n",
    "for i in range(0, n_vars):\n",
    "    guess_weights.append(rm.randint(1, 100))\n",
    "    \n",
    "guess_bias = rm.randint(1, 100)\n",
    "\n",
    "# Now, comparing prediction and actual model:\n",
    "model_str = \"y =\"\n",
    "for i in range(0, len(guess_weights)):\n",
    "    model_str += \" {}*x{} +\".format(guess_weights[i], i)\n",
    "\n",
    "model_str += \" {}\".format(guess_bias) \n",
    "\n",
    "print(\"\\nThe prediction model:\")\n",
    "print(model_str)\n",
    "\n",
    "print(\"\\nThe actual mathematical relationship is: \")\n",
    "print(rel_str)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14de2597",
   "metadata": {},
   "source": [
    "<br><br><br>\n",
    "Now that we've gotten our machine learning algorithm (it hasn't done any actual \"learning\" yet) to create a prediction model \n",
    "for us, we'll use the model to make some predictions (we can already see from above that it should be quite inaccurate)\n",
    "\n",
    "*Note: We'll be making quite a few predictions, so we'll create a function, just to cut down on repetitive code.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "92bc1d22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The first 5 PREDICTED values of y: \n",
      "['1.41E+08', '1.14E+08', '1.11E+08', '4.60E+07', '6.04E+07']\n",
      "\n",
      "\n",
      "Difference between the first 5 predicted values of y, and the first 5 ACTUAL values of y: \n",
      "['1.04E+08', '8.42E+07', '8.62E+07', '2.69E+07', '3.70E+07']\n",
      "\n",
      "\n",
      "Proportional Difference: \n",
      "['286.545%', '280.261%', '346.75%', '140.913%', '157.887%']\n"
     ]
    }
   ],
   "source": [
    "def predict(weights, bias):\n",
    "    \n",
    "    predictions = []\n",
    "\n",
    "    # Iterating across all columns\n",
    "    for i in range(0, n_instncs):\n",
    "\n",
    "        predict_val = 0\n",
    "\n",
    "        # Iterating through a specific column/instance\n",
    "        for j in range(0, n_vars):\n",
    "\n",
    "            term = weights[j]*data[j][i]\n",
    "            predict_val += term\n",
    "\n",
    "        predict_val += bias\n",
    "        predictions.append(predict_val)\n",
    "    \n",
    "    return predictions\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Displaying the first 5 PREDICTED values of y:\n",
    "\n",
    "predictions = predict(guess_weights, guess_bias) \n",
    "print(\"\\nThe first 5 PREDICTED values of y: \")\n",
    "print([\"{:.2E}\".format(n) for n in predictions[:5]])\n",
    "print(\"\\n\")\n",
    "\n",
    "\n",
    "\n",
    "# Displaying the first 5 differences (between predicted and actual): \n",
    "\n",
    "diff_first5 = [predictions[i] - dep_data[i] for i in range(0, 5)]\n",
    "print(\"Difference between the first 5 predicted values of y, and the first 5 ACTUAL values of y: \")\n",
    "print([\"{:.2E}\".format(n) for n in diff_first5])\n",
    "print(\"\\n\")\n",
    "\n",
    "\n",
    "\n",
    "# Displaying the differences as proportions of corresponding actual values:\n",
    "\n",
    "proport_diff = [diff_first5[i] / dep_data[i] for i in range(0, 5)]\n",
    "print(\"Proportional Difference: \")\n",
    "print([ \"{:g}%\".format(n*100) for n in proport_diff])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6523c314",
   "metadata": {},
   "source": [
    "<br><br><br>\n",
    "As we can see, the predictions are pretty far off, even when we've randomly generated the weights/bias, in the exact same way, as the coefficients/constant in the made up mathematical relationship (randomly picked integers between 1 and 100).\n",
    "\n",
    "So, it seems our model needs to be improved tremendously based off the first 5 predictions, but we'll calculate the proper cost of the model, just to be sure.\n",
    "\n",
    "\n",
    "*Note: Also creating a function for cost as well.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "85cee488",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The cost of our model, given its guessed weights/bias, is: 2.42E+15\n"
     ]
    }
   ],
   "source": [
    "def cost(weights, bias):\n",
    "    \n",
    "    cost = 0\n",
    "    predictions = predict(weights, bias)\n",
    "    for i in range(0, n_instncs):\n",
    "        diff = (predictions[i] - dep_data[i])**2\n",
    "        cost += diff\n",
    "    cost = cost / (2*n_instncs)\n",
    "    return cost\n",
    "\n",
    "\n",
    "the_cost = cost(guess_weights, guess_bias)\n",
    "print(\"\\nThe cost of our model, given its guessed weights/bias, is: {:.2E}\".format(the_cost))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cabc8e19",
   "metadata": {},
   "source": [
    "<br><br><br>\n",
    "As we can see above, the cost was huge, so we need to change the weights and bias, such that the cost will be lower. But how do we figure out what to change the weights and bias by, so that the cost decreases?\n",
    "\n",
    "Well, in order to do that, we need to know how the cost changes, as our weights and bias change. The GRADIENT of the cost literally tells us exactly how cost changes, as the weights and bias change. So we need to work out the gradient of cost.\n",
    "\n",
    "Now, we could figure out the gradient of cost, by just using SymPy (a popular Python module), and \n",
    "expressing the cost symbolically with SymPy symbols, then using SymPy's differentiation function. This will give us the \n",
    "gradient of cost, without having to do any calculus, or other mathematical work. \n",
    "\n",
    "HOWEVER, this project is supposed to be \"from scratch\", so I've gone ahead and done the calculus/mathematical work required,\n",
    "to figure out the gradient of cost from scratch. I then coded in that mathematical work, and ended up with the gradient of \n",
    "the cost function \"from scratch\". The mathematical work that I've gone through (that includes step by step explanation), \n",
    "can be viewed in the PDF, that's in the same folder as this code file.\n",
    "\n",
    "I've also coded in the gradient of cost just using SymPy, just for comparisons sake (and it also allows you to see\n",
    "what a particular partial derivative looks like, symbolically, if that interests you)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3e0e196",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "### Gradient of cost, from scratch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "05bed315",
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad_cost_sc(weights, bias):\n",
    "    \n",
    "    grad_cost_sc = []\n",
    "\n",
    "    # Iterate through each weight:\n",
    "    for k in range(0, n_vars):\n",
    "\n",
    "        # Calculate the partial derivative for that particular weight:\n",
    "        dC_dwk = 0\n",
    "        for i in range(0, n_instncs):\n",
    "\n",
    "            expr = 0\n",
    "            summation = 0\n",
    "            for j in range(0, n_vars):\n",
    "                 summation += (weights[j] * data[j][i])\n",
    "\n",
    "            expr = 2*data[k][i]*(summation + bias - dep_data[i])\n",
    "            dC_dwk += expr\n",
    "\n",
    "        dC_dwk = dC_dwk / (2*n_instncs)\n",
    "        grad_cost_sc.append(dC_dwk)\n",
    "\n",
    "\n",
    "\n",
    "    # Calculate the partial derivative for the bias:\n",
    "    dC_db = 0\n",
    "    for i in range(0, n_instncs):\n",
    "\n",
    "        expr = 0\n",
    "        summation = 0\n",
    "        for j in range(0, n_vars):\n",
    "             summation += (weights[j] * data[j][i])\n",
    "\n",
    "        expr = 2*(summation + bias - dep_data[i])\n",
    "        dC_db += expr\n",
    "\n",
    "    dC_db = dC_db / (2*n_instncs)\n",
    "    grad_cost_sc.append(dC_db)\n",
    "    return grad_cost_sc\n",
    "\n",
    "grad_cost = grad_cost_sc(guess_weights, guess_bias)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1ab7c602",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3204490448589.09,\n",
       " 3026392078.5395184,\n",
       " 40182769865576.53,\n",
       " 332610135414.0803,\n",
       " 3377922289789.0244,\n",
       " 35930064120.38476,\n",
       " 3448613.166440441,\n",
       " 2959381518317.2114,\n",
       " 34630837649322.734,\n",
       " 328083338.35640514,\n",
       " 37924.886622242935,\n",
       " 37770079187486.086,\n",
       " 63842178.972653754]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grad_cost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76cc6c38",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "As we can see from above, we now have the gradient of cost, but we'll want to verify it using SymPy, so let's do that."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c17ba9a",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "### Gradient of cost, expressed symbolically using SymPy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9d9d6da6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, expressing the weights and bias symbolically:\n",
    "\n",
    "w = sp.symbols(\"w0:{}\".format(n_vars))\n",
    "b = sp.symbols('b')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e198bd6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, expressing the cost symbolically:\n",
    "\n",
    "cost_sym = 0\n",
    "\n",
    "# Each lap of this loop, is an entire difference term\n",
    "for i in range(0, n_instncs):\n",
    "    \n",
    "    diff = 0\n",
    "    \n",
    "    # Each lap of this loop, is an individual term within a difference term\n",
    "    for j in range(0, n_vars):\n",
    "        diff += w[j]*data[j][i]\n",
    "        \n",
    "    diff += b - dep_data[i]\n",
    "    cost_sym += diff**2\n",
    "\n",
    "# Finally, we just need to divide by 2*n_instncs\n",
    "cost_sym = cost_sym / (2*n_instncs)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d149314e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now that we've expressed the cost symbolically, we can find its gradient, by simply using SymPy's differentiation function\n",
    "\n",
    "grad_cost_sympy = []\n",
    "\n",
    "for i in range(0, n_vars):\n",
    "    dC_dwi = sp.diff(cost_sym, w[i])\n",
    "    grad_cost_sympy.append(dC_dwi)\n",
    "\n",
    "dC_db = sp.diff(cost_sym, b)\n",
    "grad_cost_sympy.append(dC_db)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b58ee04",
   "metadata": {},
   "source": [
    "<br><br><br>\n",
    "So, we've worked out the gradient of cost symbolically, and real quick just for curiosities sake, we'll take a peek at it (purely optional, just for fun).  \n",
    "\n",
    "Unfortunately, we can't look at the entire gradient, as the expression would be gigantic (unless we use summation notation), so we'll just take a look at the first partial derivative (derivative of cost with respect to w0):\n",
    "\n",
    "*Warning: If n_vars is set to a large number, then even a single partial derivative can look quite big, as the number of terms present, will be equal to: n_vars + 2*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6969dffd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$\\displaystyle 52692.2187315113 b + 3512432063.06779 w_{0} + 2578662.64410617 w_{1} + 30.7502747847605 w_{10} + 24145538239.7407 w_{11} + 29194850640.784 w_{2} + 284644276.810988 w_{3} + 2841227051.47236 w_{4} + 29208521.746297 w_{5} + 2711.82121128947 w_{6} + 2461159883.06276 w_{7} + 29155720301.8291 w_{8} + 274436.926162413 w_{9} - 1473635844951.25$"
      ],
      "text/plain": [
       "52692.2187315113*b + 3512432063.06779*w0 + 2578662.64410617*w1 + 30.7502747847605*w10 + 24145538239.7407*w11 + 29194850640.784*w2 + 284644276.810988*w3 + 2841227051.47236*w4 + 29208521.746297*w5 + 2711.82121128947*w6 + 2461159883.06276*w7 + 29155720301.8291*w8 + 274436.926162413*w9 - 1473635844951.25"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Derivative of cost with respect to w0, symbolically:\n",
    "grad_cost_sympy[0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1d65596",
   "metadata": {},
   "source": [
    "<br><br><br>\n",
    "Awesome! That looks right, now, all we have to do, is substitute the symbolic weights/bias in these SymPy expressions, with the actual values they're equal to. \n",
    "\n",
    "To do so, we have to use the SymPy \"subs\" method, which means the weights/bias, and the values they're equal to, will need to be paired into tuples, and those tuples need to go into a list:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "be0280aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights_bias_vals = []\n",
    "\n",
    "# Iterating through each weight, putting both its symbolic form, and numeric form, into a tuple. \n",
    "for i in range(0, n_vars):\n",
    "    weights_bias_vals.append((w[i], guess_weights[i]))\n",
    "\n",
    "# Putting the bias in symbolic form, and numeric form, into a tuple\n",
    "weights_bias_vals.append((b, guess_bias))\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f4d230f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now that we have a tuple for every weight/bias, we can use the subs method: \n",
    "\n",
    "# Iterating through each partial derivative, and substituting the weights/bias for the values they're equal to:\n",
    "grad_cost_sympy_num = []\n",
    "\n",
    "for i in range(0, len(grad_cost_sympy)):\n",
    "\n",
    "    expr = grad_cost_sympy[i]\n",
    "    deriv_num = expr.subs(weights_bias_vals)\n",
    "    grad_cost_sympy_num.append(deriv_num)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c88975e",
   "metadata": {},
   "source": [
    "<br><br><br>\n",
    "Now that we have the gradient of cost in numeric form using SymPy, we can compare it to our gradient of cost that we got from\n",
    "scratch, to see if there's any differences. We'll just look at the first five partial derivatives, as the same mathematics is used for every single derivative (so if one is off, then they'll all be off)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "17366158",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Difference: -0.000488281250000000,           Proportional Difference: -1.52374069398455E-16\n",
      "Difference: -0.00000238418579101563,           Proportional Difference: -7.87798054297772E-16\n",
      "Difference: 0.0156250000000000,           Proportional Difference: 3.88848256411152E-16\n",
      "Difference: 0.000122070312500000,           Proportional Difference: 3.67007194017192E-16\n",
      "Difference: 0.00146484375000000,           Proportional Difference: 4.33652294023463E-16\n"
     ]
    }
   ],
   "source": [
    "for i in range(0, 5):\n",
    "    diff = grad_cost[i] - grad_cost_sympy_num[i]\n",
    "    print(\"Difference: {},           Proportional Difference: {}\".format(diff, diff/grad_cost_sympy_num[i]))\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e350096",
   "metadata": {},
   "source": [
    "<br><br><br>\n",
    "Interestingly, there are differences between our \"from scratch\" partial derivatives, and the ones from SymPy, although they are incredibly small, generally off by only a factor of E-15 (a quadrillionth). This is most likely due to rounding off with floats (can be determined by looking at SymPys: \"diff\" function source code), or imprecision when storing floats. \n",
    "\n",
    "Thus, with our \"from scratch\" gradient of cost having been verified as correct, we'll use it from now on, to improve\n",
    "the weights and bias.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb50d91e",
   "metadata": {},
   "source": [
    "<br><br><br>\n",
    "To improve a particular weight/bias, all we have to do, is add onto it, alpha multiplied by the associated negative partial derivative. \n",
    "\n",
    "*Note: The concept of alpha, and how to find a good value for it, is discussed in the PDF.*\n",
    "\n",
    "So for example: to change weight zero (w0), it would simply be: <br> <br>\n",
    "w0 = w0 + alpha * -(dC/dW0)\n",
    "\n",
    "Or for the bias (b), it would be: <br> <br>\n",
    "b = b + alpha * -(dC/db)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2de9212f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Improving the weights and bias\n",
    "alpha = 1/(10**12)\n",
    "\n",
    "# Iterating through each weight, changing it\n",
    "for i in range(0, len(guess_weights)):\n",
    "    guess_weights[i] = guess_weights[i] + alpha*-(grad_cost[i])\n",
    "\n",
    "# Changing the bias (partial derivative of bias, is the final element of the gradient):\n",
    "guess_bias = guess_bias + alpha*-(grad_cost[-1])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4df00c86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do 10,000 laps of training, you can DECREASE the number of laps to improve runtime, but accuracy will decrease\n",
    "for i in range(0, 10000):\n",
    "    \n",
    "    # Changing the weights\n",
    "    for j in range(0, len(guess_weights)):\n",
    "        guess_weights[j] = guess_weights[j] + alpha*-(grad_cost[j])\n",
    "        \n",
    "    # Changing the bias\n",
    "    guess_bias = guess_bias + alpha*-(grad_cost[-1])\n",
    "    \n",
    "    \n",
    "    # Updating the gradient of cost\n",
    "    grad_cost = grad_cost_sc(guess_weights, guess_bias)\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "301a336a",
   "metadata": {},
   "source": [
    "<br><br><br>\n",
    "Training complete! Let's see how much our model has improved by. First, we'll calculate the new cost:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fce02ad6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The cost of our now trained model is: 4.94E+09\n",
      "\n",
      "The old cost was 2.42E+15, so our cost has gotten 4.90E+05 times smaller\n"
     ]
    }
   ],
   "source": [
    "# Calculate new cost\n",
    "new_cost = cost(guess_weights, guess_bias)\n",
    "\n",
    "# Display new cost\n",
    "print(\"The cost of our now trained model is: {:.2E}\\n\".format(new_cost))\n",
    "\n",
    "# Display old cost, and improvement factor\n",
    "print(\"The old cost was {:.2E}, so our cost has gotten {:.2E} times smaller\".format(the_cost, (the_cost/new_cost)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb3d662f",
   "metadata": {},
   "source": [
    "<br><br><br>\n",
    "So, our cost has improved tremendously (the last time I ran the code, the cost got over a billion times smalller), it's definitely looking good so far.\n",
    "Next, we'll look at the first 5 predictions, and see how far off they were from the actual values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a2cfd317",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Difference between prediction_0 and actual_0: -131005.4381391406\n",
      "Proportional difference, as a percentage: -0.360% \n",
      "\n",
      "Difference between prediction_1 and actual_1: -61166.25034687668\n",
      "Proportional difference, as a percentage: -0.203% \n",
      "\n",
      "Difference between prediction_2 and actual_2: 29358.63032885641\n",
      "Proportional difference, as a percentage: 0.118% \n",
      "\n",
      "Difference between prediction_3 and actual_3: -41201.49051652476\n",
      "Proportional difference, as a percentage: -0.216% \n",
      "\n",
      "Difference between prediction_4 and actual_4: -92129.89540355653\n",
      "Proportional difference, as a percentage: -0.393% \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(0, 5):\n",
    "    diff = predict(guess_weights, guess_bias)[i] - dep_data[i]\n",
    "    proport_diff = diff/dep_data[i]*100\n",
    "    print(\"Difference between prediction_{} and actual_{}: {}\".format(i, i, diff))\n",
    "    print(\"Proportional difference, as a percentage: {:.3f}% \\n\".format(proport_diff))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "222cfa81",
   "metadata": {},
   "source": [
    "<br><br><br>\n",
    "The model our machine learning algorithm came up with, seems far more accurate now, off by only 0.1% or less, but just\n",
    "to get a concrete idea, we'll work out the average proportional difference (amongst all predictions):\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4797374d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "On average, a prediction will be off by about -0.059%\n"
     ]
    }
   ],
   "source": [
    "# Calculate average proportional difference\n",
    "sum_proport = 0\n",
    "for i in range(0, n_instncs):\n",
    "    diff = predict(guess_weights, guess_bias)[i] - dep_data[i]\n",
    "    proport_diff = diff/dep_data[i]\n",
    "    sum_proport += proport_diff\n",
    "\n",
    "avg_proport = (sum_proport/n_instncs)*100\n",
    "print(\"On average, a prediction will be off by about {:.3f}%\".format(avg_proport))\n",
    "                                                                 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "052840c8",
   "metadata": {},
   "source": [
    "<br><br><br>\n",
    "**There we have it! A prediction model, with an accuracy of around 99.9% on the data (off by only <0.1% or so on average, assuming 10,000 training laps were performed).**\n",
    "\n",
    "Now, it is very important to note, that **we tested the accuracy of our model, on the same exact same dataset, as the one used to train it**. This is never done in the real world, instead there'll be a training dataset, and a separate test dataset. I simply chose to have only a single dataset, for simplicity. So in reality, our model would be noticeably less accurate (more on this soon).\n",
    "\n",
    "<br>\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Now, do the weights/bias in our prediction model, actually match the coefficients and constant in the actual relationship?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b1772c46",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actual Relationship:\n",
      "y = 93*x0 + 9*x1 + 7*x2 + 82*x3 + 3*x4 + 25*x5 + 3*x6 + 93*x7 + 1*x8 + 85*x9 + 41*x10 + 27*x11 + 25 \n",
      "\n",
      "Prediction Model: (rounded off values for weights/bias)\n",
      "y = 94*x0 + 40*x1 + 7*x2 + 49*x3 + 4*x4 + 56*x5 + 10*x6 + 94*x7 + 1*x8 + 1*x9 + 95*x10 + 27*x11 + 35\n"
     ]
    }
   ],
   "source": [
    "# Display actual relationship\n",
    "print(\"Actual Relationship:\")\n",
    "print(rel_str, \"\\n\")\n",
    "\n",
    "# Display final model\n",
    "f_model_str = \"y =\"\n",
    "\n",
    "for i in range(0, len(guess_weights)):\n",
    "    f_model_str += \" {:.0f}*x{} +\".format(guess_weights[i], i)\n",
    "\n",
    "f_model_str += \" {:.0f}\".format(guess_bias) \n",
    "\n",
    "print(\"Prediction Model: (rounded off values for weights/bias)\")\n",
    "print(f_model_str)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3222a308",
   "metadata": {},
   "source": [
    "<br><br><br>\n",
    "Unfortunately, just from looking at the two equations, we can see that there are quite a few weights, that deviate quite significantly from the coefficients they're supposed to represent (be equal to). \n",
    "\n",
    "This means our prediction model, would be increasingly less accurate, as the data we're applying it to, becomes increasingly different to the data it was trained off of. (By different, I mean the scales of the variables in the data it's being applied, become increasingly different to the scales of the variables in the original training data). So if we had evaluated the model using a test dataset, that was fairly different to the training dataset, in terms of scales, then the accuracy would definitely be lower than 99.9%\n",
    "\n",
    "To help with this particular issue, we can use feature scaling, which is discussed below. \n",
    "\n",
    "Apart from that, we've finished! (:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba1dbb0b",
   "metadata": {},
   "source": [
    "<br><br><br>\n",
    "## Feedback / Improvements\n",
    "\n",
    "\n",
    "### 1. Feature Scaling\n",
    "\n",
    "Firstly, the independent variables created at the start, should be scaled appropriately (there's quite a few different ways to do so). If they aren't, then the variables that are on the largest scales, such as millions or above, are what the \n",
    "MLA trains/adapts to the most (so the weights for these larger scale variables, will become highly accurate), whereas the variables on the smallest scales, such as singles, or tenths, etc, are basically ignored/given the least priority, because they have a much smaller impact on the cost (so the weights for these smaller scale variables, will end up pretty inaccurate).\n",
    "\n",
    "- (This is actually why the final weights the MLA came up with, are different to some of the variables coefficients, because most likely, those variables are the smallest scale ones, that don't matter anywhere near as much)\n",
    "\n",
    "Additionally, if the independent variables were scaled, then the runtime of the training process, would be \n",
    "significantly improved/optimized (assuming the independent variables are on wildly different scales to begin with). \n",
    "This is because, in simple/intuitive terms, if the variables are on wildly different scales relative to each other, \n",
    "then the graph of the cost function essentially becomes warped (along certain axes), meaning the direction of steepest \n",
    "descent at most points, isn't actually pointing directly at the local minimum, but is instead off by a certain angle. \n",
    "The bigger the difference in scales between the independent the variables, the more \"off\" the direction of steepest descent\n",
    "will be (capping out at almost 90 degrees). Further explanation on this is in the PDF.\n",
    "\n",
    "\n",
    "\n",
    "### 2. Vectorization\n",
    "\n",
    "Secondly, vectorization should be utilised as much as possible, instead of what was used, whiiteration and the use of loops. This \n",
    "would significantly speed up runtime, as calculations become array based, and currently the training part of this project\n",
    "has a runtime of in the scale of tens of seconds. (Vectorization wasn't done for this project,\n",
    "as it requires an understanding of linear algebra, which would make the explanations even longer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
